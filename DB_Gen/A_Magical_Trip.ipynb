{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import platform\n",
    "import sqlite3\n",
    "import time\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.patches import ConnectionStyle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.patches import ConnectionStyle, Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib import collections\n",
    "\n",
    "\n",
    "try:\n",
    "    from importlib import metadata as importlib_metadata\n",
    "except ImportError:\n",
    "    # Backwards compatibility - importlib.metadata was added in Python 3.8\n",
    "    import importlib_metadata\n",
    "\n",
    "from PySide6.QtGui import QAction, QGuiApplication\n",
    "from PySide6.QtWidgets import QAbstractItemView, QMainWindow, QApplication, QMenu, QToolBar, QFileDialog, QTableView, QVBoxLayout, QHBoxLayout, QWidget, QSlider,  QGroupBox , QLabel , QWidgetAction, QPushButton, QSizePolicy\n",
    "from PySide6.QtCore import QAbstractTableModel, QModelIndex, QVariantAnimation, Qt\n",
    "\n",
    "\n",
    "from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "import pandas as pd\n",
    "from PySide6.QtGui import QGuiApplication\n",
    "\n",
    "from PySide6.QtCore import QAbstractTableModel, Qt, QModelIndex\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "plt.rcParams['pdf.fonttype'] =  'truetype'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\langchain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU RAM Free: 7957MB | Used: 0MB | Util   0% | Total 8188MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to monitor the download progress\n",
    "start_time = time.time()\n",
    "def download_progress(count, block_size, total_size):\n",
    "    global start_time\n",
    "    duration = time.time() - start_time\n",
    "    progress_size = int(count * block_size)\n",
    "    speed = int(progress_size / (1024 * duration))\n",
    "    percent = int(count * block_size * 100 / total_size)\n",
    "    print(f\"Downloaded {progress_size} of {total_size} bytes ({percent}% done), speed {speed} KB/s\")\n",
    "    if progress_size >= total_size:  # reset start_time for next download\n",
    "        start_time = time.time()\n",
    "\n",
    "# Function to download and extract a zip file\n",
    "def download_and_extract(target_dir, repo_link, date):\n",
    "    zip_file = target_dir+\".zip\"\n",
    "    url = quote(repo_link+\"/raw/main/\"+date + \"/\"+zip_file, safe='/:')\n",
    "\n",
    "    if not os.path.exists(target_dir):\n",
    "        if os.path.exists(zip_file):\n",
    "            extract_dir = os.path.splitext(zip_file)[0]\n",
    "            os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                total_files = len(zip_ref.infolist())\n",
    "                extracted_files = 0\n",
    "                for file in zip_ref.infolist():\n",
    "                    zip_ref.extract(file, path=extract_dir)\n",
    "                    extracted_files += 1\n",
    "                    print(f'Unzip Progress: {extracted_files / total_files * 100:.2f}%')\n",
    "        else:\n",
    "            urllib.request.urlretrieve(url, zip_file, reporthook=download_progress)\n",
    "            extract_dir = os.path.splitext(zip_file)[0]\n",
    "            os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                total_files = len(zip_ref.infolist())\n",
    "                extracted_files = 0\n",
    "                for file in zip_ref.infolist():\n",
    "                    zip_ref.extract(file, path=extract_dir)\n",
    "                    extracted_files += 1\n",
    "                    print(f'Unzip Progress: {extracted_files / total_files * 100:.2f}%')\n",
    "\n",
    "        print(f'{zip_file} has been downloaded and extracted to \\n {os.path.abspath(extract_dir)}')\n",
    "\n",
    "    # Open the directory in the file explorer\n",
    "    if platform.system() == \"Windows\":\n",
    "        os.startfile(os.path.abspath(extract_dir))\n",
    "    elif platform.system() == \"Darwin\":\n",
    "        os.system(f'open \"{os.path.abspath(extract_dir)}\"')\n",
    "    else:\n",
    "        os.system(f'xdg-open \"{os.path.abspath(extract_dir)}\"')\n",
    "\n",
    "# Function to fix a line in a CSV file\n",
    "def fix_line(line):\n",
    "    pattern = r'(?<!\")\"(?!\")'\n",
    "    fixed_line = re.sub(pattern, '', line)\n",
    "    return fixed_line\n",
    "\n",
    "# Function to fix a CSV file\n",
    "def fix_csv_file(file_path):\n",
    "    with open(file_path, 'r', newline='', encoding='ISO-8859-1') as f:\n",
    "        reader = csv.reader(f)\n",
    "        rows = list(reader)\n",
    "\n",
    "    has_extra_quotes = any('\"' in field for row in rows for field in row)\n",
    "\n",
    "    if not has_extra_quotes:\n",
    "        return\n",
    "    else:\n",
    "        print(f'File \"{file_path}\" has isolated quotes error.')\n",
    "        \n",
    "\n",
    "    with open(file_path, 'w', newline='', encoding='ISO-8859-1') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for row in rows:\n",
    "            fixed_row = [fix_line(field) for field in row]\n",
    "            writer.writerow(fixed_row)\n",
    "\n",
    "# Download the data file    \n",
    "target_dir = \"GEOROC Compilation Rock Types\"\n",
    "repo_link = \"https://github.com/GeoPyTool/GeoRocData\"\n",
    "date = \"2023-12-01\"\n",
    "download_and_extract(target_dir, repo_link, date)\n",
    "\n",
    "# Fix Quota Error in the downloaded CSV files\n",
    "files = os.listdir(target_dir)\n",
    "for file in files:\n",
    "    file_name = os.path.join(target_dir, file)\n",
    "    _, extension = os.path.splitext(file_name)\n",
    "    if extension.lower() == '.csv':\n",
    "        fix_csv_file(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Funtion to split the CSV file into three separate files\n",
    "def split_csv(file_path):\n",
    "\n",
    "    # Split the path and filename\n",
    "    dir_path, file_name = os.path.split(file_path)\n",
    "\n",
    "    print(\"Dir Path:\", dir_path)\n",
    "    print(\"Filename:\", file_name)\n",
    "\n",
    "    # Get the parent directory of output_dir\n",
    "    parent_dir = os.path.dirname(dir_path)\n",
    "\n",
    "    # Join the parent directory and the 'Splited' folder\n",
    "    splited_data_dir = os.path.join(parent_dir, 'Splited_Data')\n",
    "    splited_abbreviations_dir = os.path.join(parent_dir, 'Splited_Abbreviations')\n",
    "    splited_references_dir = os.path.join(parent_dir, 'Splited_References')\n",
    "\n",
    "    # Check if splited_dir exists, if not, create it\n",
    "    for i in [splited_data_dir, splited_abbreviations_dir, splited_references_dir]:\n",
    "        if not os.path.exists(i):\n",
    "            os.makedirs(i)\n",
    "\n",
    "    # Open the file and read the lines\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Find the start and end lines for each section\n",
    "    start_line_main = 0\n",
    "    end_line_main = next(i for i, line in enumerate(lines) if 'Abbreviations:' in line) - 2\n",
    "    start_line_abbreviations = end_line_main + 2\n",
    "    end_line_abbreviations = next(i for i, line in enumerate(lines) if 'References:' in line) - 2\n",
    "    start_line_references = end_line_abbreviations + 2\n",
    "    end_line_references = len(lines) - 1\n",
    "\n",
    "    # Read each section into a DataFrame    \n",
    "    df_data = pd.read_csv(file_path, skiprows=start_line_main, nrows=end_line_main-start_line_main, encoding='ISO-8859-1', engine='python', on_bad_lines='warn')\n",
    "    df_abbreviations = pd.read_csv(file_path, skiprows=start_line_abbreviations, nrows=end_line_abbreviations-start_line_abbreviations, encoding='ISO-8859-1', engine='python', on_bad_lines='warn')\n",
    "    df_references = pd.read_csv(file_path, skiprows=start_line_references, nrows=end_line_references-start_line_references, encoding='ISO-8859-1', engine='python', on_bad_lines='warn')\n",
    "\n",
    "    # Write each DataFrame to a separate CSV file\n",
    "    df_data.to_csv(splited_data_dir+'/'+file_name.replace('.csv','')+'_data.csv', index=False)\n",
    "    df_abbreviations.to_csv(splited_abbreviations_dir+'/'+file_name.replace('.csv','')+ '_abbreviations.csv', index=False)\n",
    "    df_references.to_csv(splited_references_dir+'/'+file_name.replace('.csv','')+ '_references.csv', index=False)\n",
    "\n",
    "    # Open the directory in the file explorer\n",
    "    if platform.system() == \"Windows\":\n",
    "        os.startfile(os.path.abspath(splited_data_dir))\n",
    "    elif platform.system() == \"Darwin\":\n",
    "        os.system(f'open \"{os.path.abspath(splited_data_dir)}\"')\n",
    "    else:\n",
    "        os.system(f'xdg-open \"{os.path.abspath(splited_data_dir)}\"')\n",
    "\n",
    "# Set the target directory name\n",
    "target_dir = 'GEOROC Compilation Rock Types'\n",
    "\n",
    "# Get all files in target_dir\n",
    "files = os.listdir(target_dir)\n",
    "\n",
    "# For each file in target_dir, run split_csv\n",
    "for file in files:\n",
    "    file_name = os.path.join(target_dir, file)\n",
    "    # Get the file extension\n",
    "    _, extension = os.path.splitext(file_name)\n",
    "    # If the file is a CSV file, run split_csv\n",
    "    if extension.lower() == '.csv':\n",
    "        split_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, here is a joke for you:\\n\\nWhat do you call a boomerang that won't come back?\\n\\nA stick.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set the target directory name\n",
    "target_dir = 'Splited_Data'\n",
    "\n",
    "# Add the filename as a new column to the CSV file\n",
    "def add_filename_as_column(file_path):    \n",
    "    \"\"\"\n",
    "    This function adds the filename as a new column to the CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the file path into directory path and file name\n",
    "    dir_path, file_name = os.path.split(file_path)\n",
    "\n",
    "    print(\"Directory Path:\", dir_path)\n",
    "    print(\"Filename:\", file_name)\n",
    "\n",
    "    # Get the parent directory of the output_dir\n",
    "    parent_dir = os.path.dirname(dir_path)\n",
    "\n",
    "    # Join the parent directory with the 'Type_Added' folder\n",
    "    output_dir = os.path.join(parent_dir, 'Type_Added')\n",
    "\n",
    "    # Check if output_dir exists, if not, create it\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Create a new file path for the modified data\n",
    "    output_path = os.path.join(output_dir, file_name.replace('.csv', '') + '_type_added.csv')\n",
    "\n",
    "    # Process the filename\n",
    "    split_file = file_name.split('_')\n",
    "    if len(split_file) > 1:\n",
    "        processed_file = split_file[1].capitalize()\n",
    "    else:\n",
    "        pass  # Skip this file if it doesn't contain an underscore\n",
    "\n",
    "    # Remove '.csv' from the processed filename\n",
    "    processed_file = processed_file.replace('.csv', '')\n",
    "\n",
    "    # Try to read the file with different encodings\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, encoding='utf-8', engine='python', on_bad_lines='warn')\n",
    "        encoding='utf-8'\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            data = pd.read_csv(file_path, encoding='ISO-8859-1', engine='python', on_bad_lines='warn')\n",
    "            encoding='ISO-8859-1'\n",
    "        except UnicodeDecodeError:\n",
    "            data = pd.read_csv(file_path, encoding='Windows-1252', engine='python', on_bad_lines='warn')\n",
    "            encoding='Windows-1252'\n",
    "\n",
    "    # Check if the first column is already 'Type'\n",
    "    if data.columns[0] != 'Type':\n",
    "        # Add 'Type' to the first column\n",
    "        data.insert(0, 'Type', processed_file)\n",
    "\n",
    "    # Write the data back to a new file\n",
    "    data.to_csv(output_path, index=False, encoding=encoding)\n",
    "\n",
    "# Get all files in target_dir\n",
    "files = os.listdir(target_dir)\n",
    "\n",
    "# For each file in target_dir, run split_csv\n",
    "for file in files:\n",
    "    file_name = os.path.join(target_dir, file)\n",
    "    # Get the file extension\n",
    "    _, extension = os.path.splitext(file_name)\n",
    "    # If the file is a CSV file, run split_csv\n",
    "    if extension.lower() == '.csv':\n",
    "        add_filename_as_column(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the target directory and database names\n",
    "target_dir = 'Type_Added'\n",
    "db_name = 'GeoRoc.db'\n",
    "\n",
    "def csv_to_db_with_check(target_dir, db_name):\n",
    "    \"\"\"\n",
    "    This function reads CSV files from the target directory and writes them to a SQLite database.\n",
    "    It also checks for and removes duplicate rows in the CSV files.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a set to store the paths of files that have been read\n",
    "    read_files = set()\n",
    "\n",
    "    # Create a SQLite database\n",
    "    conn = sqlite3.connect(db_name)\n",
    "\n",
    "    # Open the record file and read the paths of files that have been read\n",
    "    with open(db_name.replace('.db', '_') + 'imported_files.txt', 'a+') as f:\n",
    "        f.seek(0)\n",
    "        read_files = set(line.strip() for line in f)\n",
    "\n",
    "    # Iterate over all files in the target directory\n",
    "    for file in os.listdir(target_dir):\n",
    "        # Get the full path of the file\n",
    "        file_path = os.path.join(target_dir, file)\n",
    "\n",
    "        # If the file has been read, skip it\n",
    "        if file_path in read_files:\n",
    "            continue\n",
    "\n",
    "        # Get the extension of the file\n",
    "        _, extension = os.path.splitext(file_path)\n",
    "\n",
    "        # If the file is a CSV file\n",
    "        if extension.lower() == '.csv':\n",
    "            # Try to read the file with different encodings\n",
    "            try:\n",
    "                data = pd.read_csv(file_path, encoding='utf-8', engine='python', on_bad_lines='warn')\n",
    "            except UnicodeDecodeError:\n",
    "                try:\n",
    "                    data = pd.read_csv(file_path, encoding='ISO-8859-1', engine='python', on_bad_lines='warn')\n",
    "                except UnicodeDecodeError:\n",
    "                    data = pd.read_csv(file_path, encoding='Windows-1252', engine='python', on_bad_lines='warn')\n",
    "\n",
    "            # Check for duplicates\n",
    "            print(\"Duplicates before removal: \", data.duplicated().sum())\n",
    "\n",
    "            # Remove duplicates\n",
    "            data = data.drop_duplicates()\n",
    "\n",
    "            # Check for duplicates again\n",
    "            print(\"Duplicates after removal: \", data.duplicated().sum())\n",
    "\n",
    "            # Write the data to the SQLite database\n",
    "            data.to_sql('Current_Data', conn, if_exists='append', index=False)\n",
    "\n",
    "            # Record the path of the file that has been read\n",
    "            with open(db_name.replace('.db', '_') + 'imported_files.txt', 'a') as f:\n",
    "                f.write(file_path + '\\n')\n",
    "\n",
    "    # Commit the transaction and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Call the function\n",
    "csv_to_db_with_check(target_dir, db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set the target directory and database names\n",
    "target_dir = 'Splited_References'\n",
    "db_name = 'Ref.db'\n",
    "\n",
    "def csv_to_db_with_check(target_dir, db_name):\n",
    "    \"\"\"\n",
    "    This function reads CSV files from the target directory and writes them to a SQLite database.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a set to store the paths of files that have been read\n",
    "    read_files = set()\n",
    "\n",
    "    # Create a SQLite database\n",
    "    conn = sqlite3.connect(db_name)\n",
    "\n",
    "    # Open the record file and read the paths of files that have been read\n",
    "    with open(db_name.replace('.db', '_') + 'imported_files.txt', 'a+') as f:\n",
    "        f.seek(0)\n",
    "        read_files = set(line.strip() for line in f)\n",
    "\n",
    "    # Iterate over all files in the target directory\n",
    "    for file in os.listdir(target_dir):\n",
    "        # Get the full path of the file\n",
    "        file_path = os.path.join(target_dir, file)\n",
    "\n",
    "        # If the file has been read, skip it\n",
    "        if file_path in read_files:\n",
    "            continue\n",
    "\n",
    "        # Get the extension of the file\n",
    "        _, extension = os.path.splitext(file_path)\n",
    "\n",
    "        # If the file is a CSV file\n",
    "        if extension.lower() == '.csv':\n",
    "            # Try to read the file with different encodings\n",
    "            try:\n",
    "                data = pd.read_csv(file_path, encoding='utf-8', engine='python', on_bad_lines='warn', skiprows=1, header=None)\n",
    "            except UnicodeDecodeError:\n",
    "                try:\n",
    "                    data = pd.read_csv(file_path, encoding='ISO-8859-1', engine='python', on_bad_lines='warn', skiprows=1, header=None)\n",
    "                except UnicodeDecodeError:\n",
    "                    data = pd.read_csv(file_path, encoding='Windows-1252', engine='python', on_bad_lines='warn', skiprows=1, header=None)\n",
    "\n",
    "            # Write the data to the SQLite database\n",
    "            data.to_sql('Current_Data', conn, if_exists='append', index=False)\n",
    "\n",
    "            # Record the path of the file that has been read\n",
    "            with open(db_name.replace('.db', '_') + 'imported_files.txt', 'a') as f:\n",
    "                f.write(file_path + '\\n')\n",
    "\n",
    "    # Commit the transaction and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Call the function\n",
    "csv_to_db_with_check(target_dir, db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, Langsmith can be a valuable tool for testing, especially for automating repetitive tasks and identifying potential issues. Here are some ways Langsmith can help with testing:\\n\\n**1. Identifying Test Cases:**\\n\\n* **Natural Language Description:** You can define test cases in plain English using the Langsmith user interface. \\n* **Test Data Integration:** You can integrate test data directly into the test cases, eliminating manual data entry and ensuring accuracy.\\n* **Conditional Testing:** You can create conditional statements to specify specific test scenarios based on various conditions.\\n\\n**2. Test Case Execution and Reporting:**\\n\\n* **Test Case Execution:** Langsmith can run test cases automatically, saving you time and effort.\\n* **Test Case Reporting:** Langsmith generates comprehensive reports that highlight test results, pass/fail status, and detailed logs.\\n\\n**3. Identifying Potential Issues:**\\n\\n* **Error Detection:** Langsmith identifies syntax errors, logical errors, and other issues in your test scripts.\\n* **Reusability of Test Cases:** You can create reusable test cases that can be applied to multiple test scenarios.\\n* **Test Case Documentation:** Langsmith generates documentation for your test cases, including descriptions, pre-conditions, and post-conditions.\\n\\n**4. Automating Repetitive Tasks:**\\n\\n* **Data-Driven Testing:** You can use data-driven testing techniques with Langsmith to generate and execute tests based on specific datasets.\\n* **Regression Testing:** You can automate regression testing to ensure that changes to the code don't introduce new issues.\\n\\n**5. Improving Documentation:**\\n\\n* **Clear and Concise Output:** Langsmith provides clear and concise reports and logs, making it easier for you to understand and analyze test results.\\n* **Test Case Linking:** You can link related test cases together to identify issues that may be affecting multiple functionalities.\\n\\nOverall, Langsmith can be a powerful tool for testing, helping you automate tasks, identify potential issues, and improve your documentation.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a figure with aspect ratio 2:1\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# Set the aspect ratio of axes to 3:2\n",
    "ax.set_aspect(3/2)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "def download_progress(count, block_size, total_size):\n",
    "    \"\"\"\n",
    "    This function is used to track the progress of a download.\n",
    "    \"\"\"\n",
    "    global start_time\n",
    "    duration = time.time() - start_time\n",
    "    progress_size = int(count * block_size)\n",
    "    speed = int(progress_size / (1024 * duration))\n",
    "    percent = int(count * block_size * 100 / total_size)\n",
    "    print(f\"Downloaded {progress_size} of {total_size} bytes ({percent}% done), speed {speed} KB/s\")\n",
    "    if progress_size >= total_size:  # reset start_time for next download\n",
    "        start_time = time.time()\n",
    "\n",
    "\n",
    "def download_and_extract(filename, zip_file, url, extract_dir):\n",
    "    \"\"\"\n",
    "    This function downloads a zip file from a given URL, extracts it, and opens the directory in the file explorer.\n",
    "\n",
    "    Args:\n",
    "    filename (str): The name of the file to check if it exists.\n",
    "    zip_file (str): The name of the zip file to download.\n",
    "    url (str): The URL to download the zip file from.\n",
    "    extract_dir (str): The directory to extract the zip file to.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        if os.path.exists(zip_file):\n",
    "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                total_files = len(zip_ref.infolist())\n",
    "                extracted_files = 0\n",
    "                for file in zip_ref.infolist():\n",
    "                    zip_ref.extract(file, path=extract_dir)\n",
    "                    extracted_files += 1\n",
    "                    print(f'Unzip Progress: {extracted_files / total_files * 100:.2f}%')\n",
    "        else:\n",
    "            urllib.request.urlretrieve(url, zip_file, reporthook=download_progress)\n",
    "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                total_files = len(zip_ref.infolist())\n",
    "                extracted_files = 0\n",
    "                for file in zip_ref.infolist():\n",
    "                    zip_ref.extract(file, path=extract_dir)\n",
    "                    extracted_files += 1\n",
    "                    print(f'Unzip Progress: {extracted_files / total_files * 100:.2f}%')\n",
    "\n",
    "        print(f'{zip_file} has been downloaded and extracted to \\n {os.path.abspath(extract_dir)}')\n",
    "\n",
    "        # Open the directory in the file explorer\n",
    "        if platform.system() == \"Windows\":\n",
    "            os.startfile(os.path.abspath(extract_dir))\n",
    "        elif platform.system() == \"Darwin\":\n",
    "            os.system(f'open \"{os.path.abspath(extract_dir)}\"')\n",
    "        else:\n",
    "            os.system(f'xdg-open \"{os.path.abspath(extract_dir)}\"')\n",
    "\n",
    "\n",
    "def visualize_data(df, name):\n",
    "    \"\"\"\n",
    "    This function is used to visualize the data.\n",
    "    \"\"\"\n",
    "    data = (df[(df[\"ROCK TYPE\"] == name)]['Type'].value_counts())\n",
    "\n",
    "    # Set the golden ratio\n",
    "    golden_ratio = 1.618\n",
    "\n",
    "    # Set the width and height of the figure\n",
    "    fig_width = 10\n",
    "    fig_height = fig_width / golden_ratio\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "\n",
    "    # Sort the data\n",
    "    data_sorted = data.sort_values(ascending=False)\n",
    "\n",
    "    # Calculate the median of the sample quantity\n",
    "    median = data_sorted.median()\n",
    "\n",
    "    # Draw a horizontal line to represent the median using the axes method\n",
    "    ax.axhline(y=median, color='k', linestyle='--')\n",
    "\n",
    "    # Create a vertical bar chart using the axes method\n",
    "    colors = ['black' if x >= median else 'gray' for x in data_sorted.values]\n",
    "    bars = ax.bar(data_sorted.index, data_sorted.values, color=colors, alpha=0.3)\n",
    "\n",
    "    # Add the corresponding value at the highest point inside each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if height >= median:\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2, height,\n",
    "                    '{:d}'.format(int(height)), ha='center', va='bottom', rotation=90)\n",
    "\n",
    "    # Find the index of the first value below the median\n",
    "    first_below_median_index = next(i for i, x in enumerate(data_sorted.values) if x < median)\n",
    "\n",
    "    # Get the corresponding x value\n",
    "    first_below_median_x = data_sorted.index[first_below_median_index]\n",
    "\n",
    "    # Add the value of the median next to the median line\n",
    "    ax.text(first_below_median_x, median, 'Median: {:.2f}'.format(median), color='k', va='bottom')\n",
    "\n",
    "    # Set the color of the x-axis labels\n",
    "    for ticklabel, tickcolor in zip(ax.get_xticklabels(), colors):\n",
    "        ticklabel.set_color(tickcolor)\n",
    "\n",
    "    # Find the indices of all values greater than 30\n",
    "    indices_gt_30 = [i for i, x in enumerate(data_sorted.values) if x > 30]\n",
    "\n",
    "    # Display these indices and their corresponding values in the upper right part of the graph, divided into three columns\n",
    "    for idx, i in enumerate(indices_gt_30):\n",
    "        color = 'black' if data_sorted.values[i] >= median else 'gray'\n",
    "        if idx % 3 == 0:  # For indices with a modulus of 0, put them in the left column\n",
    "            ax.text(0.33, 1 - 0.05 * (idx // 3) - 0.1, ' {}: {}'.format(data_sorted.index[i], data_sorted.values[i]),\n",
    "                    color=color, horizontalalignment='right', verticalalignment='top', transform=ax.transAxes)\n",
    "        elif idx % 3 == 1:  # For indices with a modulus of 1, put them in the middle column\n",
    "            ax.text(0.66, 1 - 0.05 * (idx // 3) - 0.1, ' {}: {}'.format(data_sorted.index[i], data_sorted.values[i]),\n",
    "                    color=color, horizontalalignment='right', verticalalignment='top', transform=ax.transAxes)\n",
    "        else:  # For indices with a modulus of 2, put them in the right column\n",
    "            ax.text(1, 1 - 0.05 * (idx // 3) - 0.1, ' {}: {}'.format(data_sorted.index[i], data_sorted.values[i]),\n",
    "                    color=color, horizontalalignment='right', verticalalignment='top', transform=ax.transAxes)\n",
    "\n",
    "    # Add title and labels using the axes method\n",
    "    ax.set_title('Data Count of ' + name + ' Type')\n",
    "    ax.set_ylabel('Number of Data Entries')\n",
    "\n",
    "    # Set the right limit of the x-axis\n",
    "    ax.set_xlim(right=data_sorted.index[-1])\n",
    "\n",
    "    # Rotate the labels of the x-axis and y-axis\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "    plt.yticks(rotation=90)\n",
    "\n",
    "    # Remove the top and right frames of the graph\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # Adjust the figure automatically\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save the figure as SVG and PNG files\n",
    "    output_dir = 'Db_Stats'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    fig.savefig(output_dir + '/' + name + '_Stats.svg')\n",
    "    fig.savefig(output_dir + '/' + name + '_Stats.jpg', dpi=600)\n",
    "    data.to_csv(output_dir + '/' + name + '_List.csv')\n",
    "\n",
    "\n",
    "# Define the filename, zip file, URL, and extraction directory\n",
    "filename = \"GeoRoc.db\"\n",
    "zip_file = \"GeoRoc.zip\"\n",
    "url = \"https://github.com/GeoPyTool/GeoRocData/raw/main/GeoRoc.zip\"\n",
    "extract_dir = \"./\"\n",
    "\n",
    "# Download and extract the file\n",
    "download_and_extract(filename, zip_file, url, extract_dir)\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(filename)\n",
    "\n",
    "# Read the data from the Current_Data table\n",
    "df = pd.read_sql_query(\"SELECT * FROM Current_Data\", conn)\n",
    "\n",
    "# Visualize the data\n",
    "visualize_data(df, name='VOL')\n",
    "visualize_data(df, name='PLU')\n",
    "\n",
    "# Close the connection to the database\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_harker_diagram(filename, rock_type, major_oxides,output_dir):\n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(filename)\n",
    "\n",
    "    # Read the data from the Current_Data table\n",
    "    df = pd.read_sql_query(\"SELECT * FROM Current_Data\", conn)\n",
    "\n",
    "    # Select columns\n",
    "    selected_columns = df[[\"Type\", \"SIO2(WT%)\", \"TIO2(WT%)\", \"AL2O3(WT%)\", \"FEOT(WT%)\",  \"CAO(WT%)\", \n",
    "                    \"MGO(WT%)\", \"MNO(WT%)\", \"K2O(WT%)\", \"NA2O(WT%)\", \"P2O5(WT%)\",\"ROCK TYPE\"]]\n",
    "\n",
    "    # Filter rows where \"ROCK TYPE\" is rock_type and \"SIO2(WT%)\" is not 0\n",
    "    tag_df = selected_columns[(selected_columns[\"ROCK TYPE\"] == rock_type) & (selected_columns[\"SIO2(WT%)\"] != 0)]\n",
    "\n",
    "    # Output the count of \"Type\" values\n",
    "    print(tag_df[\"Type\"].value_counts())\n",
    "    type_counts = tag_df[\"Type\"].value_counts()\n",
    "    type_counts.to_csv(output_dir + '/'+'Harker_'+rock_type+'.csv')\n",
    "\n",
    "    # Create bi-variant plots\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "\n",
    "    # Check if tag_color_dict.json file exists\n",
    "    # 检查是否存在tag_color_dict.json文件\n",
    "    if os.path.exists('Color_Config/'+rock_type+'_color_dict.json'):\n",
    "        # 如果存在，从文件中读取tag_color_dict\n",
    "        with open('Color_Config/'+rock_type+'_color_dict.json', 'r') as f:\n",
    "            tag_color_dict = json.load(f)\n",
    "    else:\n",
    "        # 如果不存在，创建新的tag_color_dict并保存到文件中\n",
    "        type_set = set(tag_df['Type'].unique())\n",
    "        cmap = cm.get_cmap('rainbow', len(type_set))\n",
    "        tag_color_dict = {type: cmap(i) for i, type in enumerate(type_set)}\n",
    "        with open('Color_Config/'+rock_type+'_color_dict.json', 'w') as f:\n",
    "            json.dump(tag_color_dict, f) \n",
    "\n",
    "\n",
    "    # Draw Harker scatter plots\n",
    "    for i, oxide in enumerate(major_oxides):\n",
    "        # Determine position on the subplot grid\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "\n",
    "        # Create scatter plot\n",
    "        grouped = tag_df.groupby('Type')\n",
    "        for label, group in grouped:\n",
    "            axes[row, col].scatter(group ['SIO2(WT%)'], group [oxide], alpha=0.15, label=label, color=tag_color_dict[label], edgecolors='none')\n",
    "\n",
    "        newoxide =  oxide.replace('(WT%)',' wt%')\n",
    "        axes[row, col].set_xlabel('SiO2 wt%', fontsize=7)\n",
    "        axes[row, col].set_ylabel(f'{newoxide}', fontsize=7)\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        # Calculate the 1% and 9999% quantiles of the y-axis data\n",
    "        q1 = tag_df[oxide].quantile(0.01)\n",
    "        q3 = tag_df[oxide].quantile(0.9999)\n",
    "        # Calculate the interquartile range\n",
    "        iqr = q3 - q1\n",
    "        # Calculate the most concentrated range\n",
    "        lower_bound = min(tag_df[oxide])\n",
    "        upper_bound = q3 + 0.1 * iqr\n",
    "\n",
    "        # Limit the y-axis to the most concentrated range\n",
    "        if not np.isnan(lower_bound) and not np.isnan(upper_bound) and not np.isinf(lower_bound) and not np.isinf(upper_bound):\n",
    "            axes[row, col].set_ylim(lower_bound, upper_bound)\n",
    "        else:\n",
    "            print(f\"Invalid y-axis limits for row {row}, column {col}: lower_bound={lower_bound}, upper_bound={upper_bound}\")\n",
    "\n",
    "        xlim = ax.get_xlim()\n",
    "        ylim = ax.get_ylim()\n",
    "\n",
    "        # Calculate the number of data points within the view range\n",
    "        visible_points = tag_df[(tag_df['SIO2(WT%)'] >= xlim[0]) & (tag_df['SIO2(WT%)'] <= xlim[1]) & \n",
    "                                (tag_df[oxide] >= ylim[0]) & (tag_df[oxide] <= ylim[1])]\n",
    "\n",
    "        num_visible_points = len(visible_points)\n",
    "\n",
    "        # Display the number of visible data points on the plot\n",
    "        ax.text(0.05, 0.95, f'Visible points: {num_visible_points}', transform=ax.transAxes, verticalalignment='top')\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Create a legend on the right side of the whole figure\n",
    "    legend = fig.legend(loc='center left', fontsize=4, bbox_to_anchor=(1, 0.5))\n",
    "    # Save the plot, including the legend\n",
    "    plt.savefig(output_dir + '/'+'Harker_'+rock_type+'.svg')\n",
    "    plt.savefig(output_dir + '/'+'Harker_'+rock_type+'.png', dpi=600)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Set the parameters\n",
    "filename = 'GeoRoc.db'\n",
    "rock_type = 'VOL'\n",
    "color_config_dir = 'Color_Config'\n",
    "if not os.path.exists(color_config_dir):\n",
    "    os.makedirs(color_config_dir)\n",
    "output_dir = 'Harker'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "# List of major oxides for y-axis\n",
    "major_oxides = [\"TIO2(WT%)\", \"AL2O3(WT%)\", \"FEOT(WT%)\",  \"CAO(WT%)\", \n",
    "                \"MGO(WT%)\", \"MNO(WT%)\", \"K2O(WT%)\", \"NA2O(WT%)\", \"P2O5(WT%)\"]\n",
    "# generate_harker_diagram(filename, rock_type, major_oxides)\n",
    "generate_harker_diagram(filename,'VOL', major_oxides,output_dir)\n",
    "generate_harker_diagram(filename,'PLU', major_oxides,output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(current_directory+'/Plot_Json/tas_cord.json', 'r', encoding='utf-8') as file:\n",
    "    cord = json.load(file)\n",
    "\n",
    "# 创建一个宽高比为2:1的figure\n",
    "fig = plt.figure(figsize=(10, 5))     \n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# 设置axes的宽高比为3:2\n",
    "ax.set_aspect(3/2)\n",
    "\n",
    "def TAS_all(filename, rock_type, output_dir):\n",
    "    # 连接到数据库\n",
    "    conn = sqlite3.connect(filename)\n",
    "\n",
    "    # Read the data from the TAS_Data table\n",
    "    df = pd.read_sql_query(\"SELECT * FROM Current_Data\", conn)\n",
    "\n",
    "    selected_columns = df[[\"Type\", \"SIO2(WT%)\", \"NA2O(WT%)\", \"K2O(WT%)\",\"ROCK TYPE\"]]\n",
    "\n",
    "    # 筛选'ROCK TYPE'为tag的行，并且去掉不含SiO2的行\n",
    "    tag_df = selected_columns[(selected_columns[\"ROCK TYPE\"] == rock_type) & (selected_columns[\"SIO2(WT%)\"] != 0)]\n",
    "    tag_df['ALL_Alkaline']= tag_df[\"NA2O(WT%)\"]+tag_df[\"K2O(WT%)\"]\n",
    "\n",
    "    # 检查是否存在tag_color_dict.json文件\n",
    "    if os.path.exists('Color_Config/'+rock_type+'_color_dict.json'):\n",
    "        # 如果存在，从文件中读取tag_color_dict\n",
    "        with open('Color_Config/'+rock_type+'_color_dict.json', 'r') as f:\n",
    "            tag_color_dict = json.load(f)\n",
    "    else:\n",
    "        # 如果不存在，创建新的tag_color_dict并保存到文件中\n",
    "        type_set = set(tag_df['Type'].unique())\n",
    "        cmap = cm.get_cmap('rainbow', len(type_set))\n",
    "        tag_color_dict = {type: cmap(i) for i, type in enumerate(type_set)}\n",
    "        with open('Color_Config/'+rock_type+'_color_dict.json', 'w') as f:\n",
    "            json.dump(tag_color_dict, f) \n",
    "\n",
    "\n",
    "    # 输出'Type'的取值个数\n",
    "    print(tag_df['Type'].value_counts())\n",
    "    # 计算'Type'的取值个数\n",
    "    type_counts = tag_df['Type'].value_counts()\n",
    "    type_counts.to_csv(output_dir+'/TAS_'+rock_type+'_type_counts_tag.csv')\n",
    "\n",
    "    # 绘制TAS图解散点图\n",
    "    # label = tag_df['Type']\n",
    "    # 假设df是包含'x', 'y', 'label'列的DataFrame\n",
    "    labelled_groups = set()\n",
    "    grouped = tag_df.groupby('Type')\n",
    "\n",
    "    for label, group in grouped:\n",
    "        ax.scatter(group[\"SIO2(WT%)\"], group['ALL_Alkaline'], alpha=0.05, label=label, color=tag_color_dict[label], edgecolors='none')\n",
    "        center_x = group[\"SIO2(WT%)\"].mean()\n",
    "        center_y = group['ALL_Alkaline'].mean()\n",
    "        if label not in labelled_groups:\n",
    "            if 35 <= center_x <= 80 and 0 <= center_y <= 17.6478:\n",
    "                # ax.text(center_x, center_y, label, fontsize=9)\n",
    "                pass\n",
    "            else:\n",
    "                print(label+\" Coordinates are out of bounds\")\n",
    "                labelled_groups.add(label)\n",
    "\n",
    "    # 绘制TAS图解边界线条\n",
    "    # Draw TAS diagram boundary lines\n",
    "    for line in cord['coords'].values():\n",
    "        x_coords = [point[0] for point in line]\n",
    "        y_coords = [point[1] for point in line]\n",
    "        ax.plot(x_coords, y_coords, color='black', linewidth=0.3)\n",
    "        \n",
    "\n",
    "    # 在TAS图解中添加岩石种类标签\n",
    "    # Add rock type labels in TAS diagram\n",
    "    for label, coords in cord['coords'].items():\n",
    "        x_coords = [point[0] for point in coords]\n",
    "        y_coords = [point[1] for point in coords]\n",
    "        x_center = sum(x_coords) / len(x_coords)\n",
    "        y_center = sum(y_coords) / len(y_coords)\n",
    "        ax.text(x_center, y_center, label, ha='center', va='center', bbox=dict(facecolor='white', alpha=0.3), fontsize=12)\n",
    "\n",
    "    ax.set_xlabel(r\"SiO2\", fontsize=16)\n",
    "    ax.set_ylabel(r\"Na2O+K2O\", fontsize=16)\n",
    "    ax.set_title(r\"Extended TAS Diagram\", fontsize=16)\n",
    "    ax.set_xlim(35,80)\n",
    "    ax.set_ylim(0,17.647826086956513)  \n",
    "\n",
    "    ax.tick_params(axis='both', labelsize=12)\n",
    "    legend = ax.legend(loc='upper left', fontsize=4, bbox_to_anchor=(1, 1))\n",
    "\n",
    "    # 获取当前的视域范围\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # 计算在视域范围内的数据点的数量\n",
    "    visible_points = tag_df[(tag_df[\"SIO2(WT%)\"] >= xlim[0]) & (tag_df[\"SIO2(WT%)\"] <= xlim[1]) & \n",
    "                            (tag_df['ALL_Alkaline'] >= ylim[0]) & (tag_df['ALL_Alkaline'] <= ylim[1])]\n",
    "\n",
    "    num_visible_points = len(visible_points)\n",
    "\n",
    "    # 在图上显示可见的数据点的数量\n",
    "    ax.text(0.05, 0.95, f'Visible points: {num_visible_points}', transform=ax.transAxes, verticalalignment='top')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    # 保存图，包含图例\n",
    "    fig.savefig(output_dir+'/TAS_figure'+rock_type+'.svg')\n",
    "    fig.savefig(output_dir+'/TAS_figure'+rock_type+'.png', dpi=600)\n",
    "    # plt.show()\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "# 文件名\n",
    "filename = 'GeoRoc.db'\n",
    "rock_type = 'VOL'\n",
    "color_config_dir = 'Color_Config'\n",
    "if not os.path.exists(color_config_dir):\n",
    "    os.makedirs(color_config_dir)\n",
    "output_dir = 'TAS'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "TAS_all(filename, 'VOL', output_dir = 'TAS')\n",
    "TAS_all(filename, 'PLU', output_dir = 'TAS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def TAS_base(filename = 'GeoRoc.db',rock_type = 'VOL',output_dir='TAS'):\n",
    "\n",
    "    result_list = [['Label','Probability']]\n",
    "    \n",
    "    # 创建一个指定宽高比的figure\n",
    "    fig = plt.figure(figsize=(10, 10))     \n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    # 设置axes的宽高比为3:2\n",
    "    # ax.set_aspect(2/3)\n",
    "\n",
    "    # Record the start time\n",
    "    begin_time = time.time()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 获取当前文件的绝对路径\n",
    "    current_file_path = os.path.abspath(__file__)\n",
    "\n",
    "    # 获取当前文件的目录\n",
    "    current_directory = os.path.dirname(current_file_path)\n",
    "    # 改变当前工作目录\n",
    "    os.chdir(current_directory)\n",
    "\n",
    "\n",
    "    with open(current_directory+'/Plot_Json/tas_cord.json', 'r', encoding='utf-8') as file:\n",
    "        cord = json.load(file)\n",
    "\n",
    "\n",
    "    # 连接到数据库\n",
    "    conn = sqlite3.connect(filename)\n",
    "\n",
    "    # Read the data from the TAS_Data table\n",
    "    df = pd.read_sql_query(\"SELECT * FROM Current_Data\", conn)\n",
    "\n",
    "    selected_columns = df[[\"Type\", \"SIO2(WT%)\", \"NA2O(WT%)\", \"K2O(WT%)\",\"ROCK TYPE\"]]\n",
    "\n",
    "    # 筛选'ROCK TYPE'为tag的行，并且去掉不含SiO2的行\n",
    "    tag_df = selected_columns[(selected_columns[\"ROCK TYPE\"] == rock_type) & (selected_columns[\"SIO2(WT%)\"] != 0)]\n",
    "    tag_df['ALL_Alkaline']= tag_df[\"NA2O(WT%)\"]+tag_df[\"K2O(WT%)\"]\n",
    "\n",
    "    # 检查是否存在tag_color_dict.json文件\n",
    "    if os.path.exists('Color_Config/'+rock_type+'_color_dict.json'):\n",
    "        # 如果存在，从文件中读取tag_color_dict\n",
    "        with open('Color_Config/'+rock_type+'_color_dict.json', 'r') as f:\n",
    "            tag_color_dict = json.load(f)\n",
    "    else:\n",
    "        # 如果不存在，创建新的tag_color_dict并保存到文件中\n",
    "        type_set = set(tag_df['Type'].unique())\n",
    "        cmap = cm.get_cmap('rainbow', len(type_set))\n",
    "        tag_color_dict = {type: cmap(i) for i, type in enumerate(type_set)}\n",
    "        with open('Color_Config/'+rock_type+'_color_dict.json', 'w') as f:\n",
    "            json.dump(tag_color_dict, f)    \n",
    "    \n",
    "    # 检查是否存在'TAS_Base_' + tag + '_Withlines.pkl'文件\n",
    "    if os.path.exists(output_dir+'/'+'TAS_Base_' + rock_type + '_Withlines.pkl'):\n",
    "        # 如果存在，从文件中读取tag_color_dict\n",
    "        with open(output_dir+'/'+'TAS_Base_' + rock_type + '_Withlines.pkl', 'rb') as f:\n",
    "            fig = pickle.load(f)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "        # 输出'Type'的取值个数\n",
    "        # print(tag_df['Type'].value_counts())\n",
    "        # 计算'Type'的取值个数\n",
    "        type_counts = tag_df['Type'].value_counts()\n",
    "        type_counts.to_csv(output_dir+'/'+'TAS_type_counts_'+ rock_type +'_.csv')\n",
    "\n",
    "        # 绘制TAS图解散点图\n",
    "        # label = tag_df['Type']\n",
    "        # 假设df是包含'x', 'y', 'label'列的DataFrame\n",
    "        labelled_groups = set()\n",
    "        grouped = tag_df.groupby('Type')\n",
    "\n",
    "        label_locations = {}\n",
    "        highest_y = 0\n",
    "\n",
    "        for label, group in grouped:\n",
    "            x = group[\"SIO2(WT%)\"]\n",
    "            y = group['ALL_Alkaline']\n",
    "            center_x = x.mean()\n",
    "            center_y = y.mean()\n",
    "            \n",
    "            if label not in labelled_groups:            \n",
    "                labelled_groups.add(label)\n",
    "                \n",
    "                if 35 <= center_x <= 80 and 0 <= center_y <= 17.6478:\n",
    "                    \n",
    "                    data_amount = len(x)\n",
    "                    # print(label, data_amount)\n",
    "                    if(data_amount>30):\n",
    "\n",
    "                        original_color =  mcolors.to_rgba(tag_color_dict[label])\n",
    "\n",
    "                        # 定义一个基数，这个基数可以根据具体需求来调整\n",
    "                        base = 0.08\n",
    "                        # 计算透明度\n",
    "                        alpha = base / np.log10(data_amount/10)             \n",
    "                        \n",
    "                        label_locations[label] = [center_x,center_y,original_color,alpha]\n",
    "                        ax.scatter(x, y, color = original_color, edgecolors='none',  alpha = alpha)\n",
    "\n",
    "                        # Record the end time\n",
    "                        tmp_time = time.time()\n",
    "\n",
    "                        # Calculate the time taken\n",
    "                        time_taken = tmp_time - start_time\n",
    "                        start_time = tmp_time\n",
    "\n",
    "                        \n",
    "                        print(f\"{label} Data amount is {data_amount}, Alpha is {alpha:.3f}, Time taken: {time_taken:.3f} seconds\")\n",
    "                    \n",
    "                        \n",
    "                else:\n",
    "                    # print(label+\" Coordinates are out of bounds\")\n",
    "                    pass\n",
    "\n",
    "        \n",
    "        # 根据 center_x 对 label_locations 进行排序\n",
    "        label_locations = dict(sorted(label_locations.items(), key=lambda item: item[1][0]))\n",
    "\n",
    "        # 定义一个列表，包含n个值\n",
    "        # 定义一个列表，步长为0.8\n",
    "        values = list(np.arange(0, 7, 0.8))\n",
    "        ha_list = [\"left\",\"right\", \"center\", \"left\",\"right\", \"center\"]\n",
    "        va_list = [\"bottom\", \"center\", \"top\"]\n",
    "        for i, (label, (center_x, center_y, original_color,alpha)) in enumerate(label_locations.items()):\n",
    "            # 根据索引在 values 中轮流取值\n",
    "            value = values[i % len(values)]\n",
    "            ha = ha_list[i % len(ha_list)]\n",
    "            va = va_list[i % len(va_list)]\n",
    "            if(label =='Phonotephrite'):\n",
    "                ha = 'left'\n",
    "            # 在图中绘制文本\n",
    "            used_x = center_x\n",
    "            used_y = 18+value\n",
    "            if highest_y <= used_y:\n",
    "                highest_y = used_y\n",
    "            ax.text(used_x, used_y, label, fontsize=11.5, color='k', \n",
    "                bbox=dict(facecolor=original_color, edgecolor=None, alpha= 0.3, pad=2),rotation = 0,\n",
    "                horizontalalignment=ha, verticalalignment=va,\n",
    "                            rotation_mode=\"anchor\")\n",
    "\n",
    "            # 添加一根从文本到中心点的虚线\n",
    "            arrowstyle = '-|>'  # 箭头样式\n",
    "            connectionstyle = ConnectionStyle(\"Arc3,rad=-0.3\")  # 连接样式\n",
    "            ax.annotate('', xy=(center_x, center_y), xytext=(used_x, used_y),\n",
    "                        arrowprops=dict(arrowstyle=arrowstyle, connectionstyle=connectionstyle, linestyle='dashed', color=original_color, alpha=0.3))\n",
    "\n",
    "        # 绘制TAS图解边界线条\n",
    "        # Draw TAS diagram boundary lines\n",
    "        for line in cord['coords'].values():\n",
    "            x_coords = [point[0] for point in line]\n",
    "            y_coords = [point[1] for point in line]\n",
    "            ax.plot(x_coords, y_coords, color='black', linewidth=0.3)\n",
    "            \n",
    "\n",
    "        # 在TAS图解中添加岩石种类标签\n",
    "        # Add rock type labels in TAS diagram\n",
    "        for label, coords in cord['coords'].items():\n",
    "            x_coords = [point[0] for point in coords]\n",
    "            y_coords = [point[1] for point in coords]\n",
    "            x_center = sum(x_coords) / len(x_coords)\n",
    "            y_center = sum(y_coords) / len(y_coords)\n",
    "            ax.text(x_center, y_center, label, ha='center', va='center', bbox=dict(facecolor='white', alpha=0.3), fontsize=14)\n",
    "\n",
    "        ax.set_xlabel(\"SiO2\", fontsize=14)\n",
    "        ax.set_ylabel(\"Na2O+K2O\", fontsize=14)\n",
    "        ax.set_title(\"Extended TAS Diagram\", fontsize=14)\n",
    "        ax.set_xlim(35,80)\n",
    "        # ax.set_ylim(0,17.647826086956513)  \n",
    "        # 在 y=17.647826086956513 的位置画一条横线\n",
    "        ax.axhline(17.647826086956513, linestyle='-', color='black', linewidth=0.3)\n",
    "        print(highest_y)\n",
    "        ax.set_ylim(0,highest_y+1)  \n",
    "        # 设置y轴的刻度\n",
    "        ax.set_yticks(range(0, 18))\n",
    "\n",
    "        ax.tick_params(axis='both', labelsize=12)\n",
    "        # legend = ax.legend(loc='upper left', fontsize=4, bbox_to_anchor=(1, 1))\n",
    "\n",
    "        # 获取当前的视域范围\n",
    "        xlim = ax.get_xlim()\n",
    "        ylim = ax.get_ylim()\n",
    "\n",
    "        # 计算在视域范围内的数据点的数量\n",
    "        visible_points = tag_df[(tag_df[\"SIO2(WT%)\"] >= xlim[0]) & \n",
    "                                (tag_df[\"SIO2(WT%)\"] <= xlim[1]) & \n",
    "                                (tag_df['ALL_Alkaline'] >= ylim[0]) & \n",
    "                                (tag_df['ALL_Alkaline'] <= ylim[1])]\n",
    "\n",
    "        num_visible_points = len(visible_points)\n",
    "\n",
    "        # 在图上显示可见的数据点的数量\n",
    "        ax.text(0.05, 0.95, f'Used points: {num_visible_points}', transform=ax.transAxes, verticalalignment='top',fontsize=14)\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        with open(output_dir+'/'+'TAS_Base_' + rock_type + '_Withlines.pkl', 'wb') as f:\n",
    "            pickle.dump(fig, f)\n",
    "\n",
    "    all_end_time = time.time()\n",
    "\n",
    "    all_time_taken = all_end_time - begin_time\n",
    "\n",
    "\n",
    "    # 保存图，包含图例\n",
    "    # 创建存图的文件夹\n",
    "    fig.savefig(output_dir+'/'+'TAS_Base_' + rock_type + '.svg')\n",
    "    # fig.savefig(output_dir+'/'+'TAS_Base_' + rock_type + '.pdf')\n",
    "    fig.savefig(output_dir+'/'+'TAS_Base_' + rock_type + '.jpg', dpi=600)\n",
    "    \n",
    "    conn.close()\n",
    "    print(f\"All time taken: {all_time_taken:.3f} seconds\")\n",
    "    return(fig)\n",
    "    \n",
    "\n",
    "# 文件名\n",
    "filename = 'GeoRoc.db'\n",
    "rock_type = 'VOL'\n",
    "color_config_dir = 'Color_Config'\n",
    "if not os.path.exists(color_config_dir):\n",
    "    os.makedirs(color_config_dir)\n",
    "output_dir = 'TAS'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "TAS_base(filename, 'VOL', output_dir = 'TAS')\n",
    "TAS_base(filename, 'PLU', output_dir = 'TAS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Pearce_base(filename = 'GeoRoc.db',Type = 'Granite',output_dir='Pearce'):\n",
    "\n",
    "    result_list = [['Label','Probability']]\n",
    "    \n",
    "\n",
    "    # Suppose you want a 2x2 grid of subplots\n",
    "    fig, ax_list = plt.subplots(2, 2, figsize=(10, 10), sharex=False, sharey=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # 设置axes的宽高比为3:2\n",
    "    # ax.set_aspect(2/3)\n",
    "\n",
    "    # Record the start time\n",
    "    begin_time = time.time()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 获取当前文件的绝对路径\n",
    "    current_file_path = os.path.abspath(__file__)\n",
    "\n",
    "    # 获取当前文件的目录\n",
    "    current_directory = os.path.dirname(current_file_path)\n",
    "    # 改变当前工作目录\n",
    "    os.chdir(current_directory)\n",
    "\n",
    "\n",
    "    with open(current_directory+'/Plot_Json/pearce_cord.json', 'r', encoding='utf-8') as file:\n",
    "        cord = json.load(file)\n",
    "\n",
    "\n",
    "    # 连接到数据库\n",
    "    conn = sqlite3.connect(filename)\n",
    "\n",
    "    # Read the data from the Pearce_Data table\n",
    "    df = pd.read_sql_query(\"SELECT * FROM Current_Data\", conn)\n",
    "\n",
    "    Elements_List =[\"Y(PPM)\", \"NB(PPM)\", \"RB(PPM)\", \"YB(PPM)\", \"TA(PPM)\"]\n",
    "    # 筛选'Type'为Type的行，并且去掉数据不完整的行\n",
    "    selected_columns = df[[\"Type\", \"TECTONIC SETTING\", \"Y(PPM)\", \"NB(PPM)\", \"RB(PPM)\", \"YB(PPM)\", \"TA(PPM)\"]]\n",
    "    conditions = (selected_columns[\"Type\"] == Type)\n",
    "    for element in Elements_List:\n",
    "        conditions = conditions & (selected_columns[element] != 0)\n",
    "    tag_df = selected_columns[conditions]\n",
    "\n",
    "\n",
    "\n",
    "    def title_except_in_parentheses(s):\n",
    "        parts = s.split('(')\n",
    "        return parts[0].title() + '(' + parts[1].upper() if len(parts) > 1 else parts[0].title()\n",
    "\n",
    "    tag_df = tag_df.rename(columns=title_except_in_parentheses)\n",
    "\n",
    "    tag_df[\"Y+Nb(PPM)\"] = tag_df[\"Y(PPM)\"] + tag_df[\"Nb(PPM)\"]\n",
    "    tag_df[\"Yb+Ta(PPM)\"] = tag_df[\"Yb(PPM)\"] + tag_df[\"Ta(PPM)\"]\n",
    "\n",
    "    # print(tag_df)\n",
    "\n",
    "    target_x_list = ['Y+Nb(PPM)','Yb+Ta(PPM)','Y(PPM)','Yb(PPM)']\n",
    "    target_y_list = ['Rb(PPM)','Rb(PPM)','Nb(PPM)','Ta(PPM)']   \n",
    "    limit_x_lit=[[0.1,2500],[0 , 300],[0 , 1000],[0.05 , 100]]\n",
    "    limit_y_lit=[[0.5,9884],[1 , 7000],[0.5 , 2040],[0.05 , 100]]\n",
    "    \n",
    "\n",
    "\n",
    "    # 检查是否存在tag_color_dict.json文件\n",
    "    if os.path.exists('Color_Config/'+Type+'_color_dict.json'):\n",
    "        # 如果存在，从文件中读取tag_color_dict\n",
    "        with open('Color_Config/'+Type+'_color_dict.json', 'r') as f:\n",
    "            tag_color_dict = json.load(f)\n",
    "    else:\n",
    "        # 输出Tectonic Setting的取值个数\n",
    "        print(tag_df[\"Tectonic Setting\"].value_counts())\n",
    "        # 如果不存在，创建新的tag_color_dict并保存到文件中\n",
    "        type_set = set(tag_df[\"Tectonic Setting\"].unique())\n",
    "        cmap = cm.get_cmap('rainbow', len(type_set))\n",
    "        tag_color_dict = {type: cmap(i) for i, type in enumerate(type_set)}\n",
    "        with open('Color_Config/'+Type+'_color_dict.json', 'w') as f:\n",
    "            json.dump(tag_color_dict, f)    \n",
    "    \n",
    "    # 检查是否存在'Pearce_Base_' + tag + '_Withlines.pkl'文件\n",
    "    if os.path.exists(output_dir+'/'+'Pearce_Base_' + Type + '_Withlines.pkl'):\n",
    "        # 如果存在，从文件中读取tag_color_dict\n",
    "        with open(output_dir+'/'+'Pearce_Base_' + Type + '_Withlines.pkl', 'rb') as f:\n",
    "            fig = pickle.load(f)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "        \n",
    "        # 绘制Pearce图解散点图\n",
    "        # label = tag_df[\"Tectonic Setting\"]\n",
    "        # 假设df是包含'x', 'y', 'label'列的DataFrame\n",
    "        labelled_groups = set()\n",
    "        grouped = tag_df.groupby(\"Tectonic Setting\")\n",
    "\n",
    "        label_locations = {}\n",
    "        highest_y = 0\n",
    "\n",
    "\n",
    "        # 遍历ax_list和target_y_list\n",
    "        for ax, target_x,target_y, limit_x,limit_y in itertools.zip_longest(ax_list.flatten(), target_x_list, target_y_list , limit_x_lit, limit_y_lit):\n",
    "\n",
    "            for label, group in grouped:\n",
    "                x = group[target_x]\n",
    "                y = group[target_y]\n",
    "                center_x = x.mean()\n",
    "                center_y = y.mean()\n",
    "                \n",
    "                if label not in labelled_groups:            \n",
    "                    labelled_groups.add(label)\n",
    "                    \n",
    "                data_amount = len(x)\n",
    "                # print(label, data_amount)\n",
    "                if(data_amount>30):\n",
    "\n",
    "                    original_color =  mcolors.to_rgba(tag_color_dict[label])\n",
    "\n",
    "                    # 定义一个基数，这个基数可以根据具体需求来调整\n",
    "                    base = 0.08\n",
    "                    # 计算透明度\n",
    "                    alpha = base / np.log10(data_amount/10)             \n",
    "                    \n",
    "                    label_locations[label] = [center_x,center_y,original_color,alpha]\n",
    "\n",
    "                    ax.scatter(x, y, color = original_color, edgecolors='none',  alpha = alpha)\n",
    "                    ax.set_xlabel(target_x, fontsize=14)\n",
    "                    ax.set_ylabel(target_y, fontsize=14)\n",
    "\n",
    "\n",
    "                    # Remove NaN and Inf values\n",
    "                    x = x[~np.isnan(x) & ~np.isinf(x)]\n",
    "                    y = y[~np.isnan(y) & ~np.isinf(y)]\n",
    "\n",
    "                    # # Check the number of unique values\n",
    "                    # if len(np.unique(x)) > 1:\n",
    "                    #     x_lower, x_upper = np.percentile(x, [5, 95])\n",
    "                    # else:\n",
    "                    #     x_lower, x_upper = x[0] - 1, x[0] + 1\n",
    "\n",
    "                    # if len(np.unique(y)) > 1:\n",
    "                    #     y_lower, y_upper = np.percentile(y, [5, 95])\n",
    "                    # else:\n",
    "                    #     y_lower, y_upper = y[0] - 1, y[0] + 1\n",
    "\n",
    "                    # # Set the limits of x and y axes\n",
    "                    # ax.set_xlim(x_lower, x_upper)\n",
    "                    # ax.set_ylim(y_lower, y_upper)\n",
    "\n",
    "                    ax.set_xlim(limit_x[0],limit_x[1])\n",
    "                    ax.set_ylim(limit_y[0],limit_y[1])                \n",
    "                    ax.set_xscale('log')\n",
    "                    ax.set_yscale('log')\n",
    "                    ax.set_aspect('equal') \n",
    "                    ax.tick_params(axis='both', labelsize=12)    \n",
    "                    # 旋转y轴的标签\n",
    "                    ax.yaxis.set_tick_params(rotation=90)\n",
    "\n",
    "                    # Record the end time\n",
    "                    tmp_time = time.time()\n",
    "\n",
    "                    # Calculate the time taken\n",
    "                    time_taken = tmp_time - start_time\n",
    "                    start_time = tmp_time\n",
    "                    \n",
    "                    print(f\"{label} Data amount is {data_amount}, Alpha is {alpha:.3f}, Time taken: {time_taken:.3f} seconds\")      \n",
    "                                                 \n",
    "\n",
    "                try:\n",
    "                    # 获取当前ax对象中的所有数据点\n",
    "                    for child in ax.get_children():\n",
    "                        # 检查这个子对象是否是一个散点图的集合\n",
    "                        if isinstance(child, collections.PathCollection):\n",
    "                            # 获取当前透明度\n",
    "                            current_alpha = child.get_alpha()\n",
    "                            # 获取数据点的数量\n",
    "                            num_points = child.get_sizes().size\n",
    "                            # 根据当前透明度和数据点的数量设置新的透明度\n",
    "                            if current_alpha is not None:\n",
    "                                if num_points <1000:  # 如果数据点的数量大于100\n",
    "                                    child.set_alpha(min(current_alpha * 2, 0.3))  # 提高透明度，但不超过1\n",
    "                                elif num_points >3000:  # 如果数据点的数量小于50\n",
    "                                    child.set_alpha(max(current_alpha / 2, 0.005))  # 降低透明度，但不低于0.01\n",
    "\n",
    "                except KeyError:\n",
    "                    pass\n",
    "        \n",
    "            # 根据 center_x 对 label_locations 进行排序\n",
    "            label_locations = dict(sorted(label_locations.items(), key=lambda item: item[1][0]))\n",
    "\n",
    "            # # 定义一个列表，包含n个值\n",
    "            # # 定义一个列表，步长为0.8\n",
    "            # values = list(np.arange(0, 7, 0.8))\n",
    "            # ha_list = [\"left\",\"right\", \"center\", \"left\",\"right\", \"center\"]\n",
    "            # va_list = [\"bottom\", \"center\", \"top\"]\n",
    "            # for i, (label, (center_x, center_y, original_color,alpha)) in enumerate(label_locations.items()):\n",
    "            #     # 根据索引在 values 中轮流取值\n",
    "            #     value = values[i % len(values)]\n",
    "            #     ha = ha_list[i % len(ha_list)]\n",
    "            #     va = va_list[i % len(va_list)]\n",
    "            #     # 在图中绘制文本\n",
    "            #     used_x = center_x\n",
    "            #     used_y = 18+value\n",
    "            #     if highest_y <= used_y:\n",
    "            #         highest_y = used_y\n",
    "            #     ax.text(used_x, used_y, label, fontsize=11.5, color='k', \n",
    "            #         bbox=dict(facecolor=original_color, edgecolor=None, alpha= 0.3, pad=2),rotation = 90,\n",
    "            #         horizontalalignment=ha, verticalalignment=va,\n",
    "            #                     rotation_mode=\"anchor\")\n",
    "\n",
    "            #     # 添加一根从文本到中心点的虚线\n",
    "            #     arrowstyle = '-|>'  # 箭头样式\n",
    "            #     connectionstyle = ConnectionStyle(\"Arc3,rad=-0.3\")  # 连接样式\n",
    "            #     ax.annotate('', xy=(center_x, center_y), xytext=(used_x, used_y),\n",
    "            #                 arrowprops=dict(arrowstyle=arrowstyle, connectionstyle=connectionstyle, linestyle='dashed', color=original_color, alpha=0.3))\n",
    "\n",
    "            # # 绘制Pearce图解边界线条\n",
    "            # # Draw Pearce diagram boundary lines\n",
    "            # for line in cord['coords'].values():\n",
    "            #     x_coords = [point[0] for point in line]\n",
    "            #     y_coords = [point[1] for point in line]\n",
    "            #     ax.plot(x_coords, y_coords, color='black', linewidth=0.3)\n",
    "            \n",
    "\n",
    "            # # 在Pearce图解中添加岩石种类标签\n",
    "            # # Add Type labels in Pearce diagram\n",
    "            # for label, coords in cord['coords'].items():\n",
    "            #     x_coords = [point[0] for point in coords]\n",
    "            #     y_coords = [point[1] for point in coords]\n",
    "            #     x_center = sum(x_coords) / len(x_coords)\n",
    "            #     y_center = sum(y_coords) / len(y_coords)\n",
    "            #     ax.text(x_center, y_center, label, ha='center', va='center', bbox=dict(facecolor='white', alpha=0.3), fontsize=14)\n",
    "\n",
    "\n",
    "            # ax.set_title(\"Extended Pearce Diagram\", fontsize=14)\n",
    "            # ax.set_xlim(35,80)\n",
    "            # ax.set_ylim(0,17.647826086956513)  \n",
    "            # # 在 y=17.647826086956513 的位置画一条横线\n",
    "            # ax.axhline(17.647826086956513, linestyle='-', color='black', linewidth=0.3)\n",
    "            # print(highest_y)\n",
    "            # ax.set_ylim(0,highest_y+1)  \n",
    "            # # 设置y轴的刻度\n",
    "            # ax.set_yticks(range(0, 18))\n",
    "\n",
    "            # legend = ax.legend(loc='upper left', fontsize=4, bbox_to_anchor=(1, 1))\n",
    "\n",
    "            # # 获取当前的视域范围\n",
    "            # xlim = ax.get_xlim()\n",
    "            # ylim = ax.get_ylim()\n",
    "\n",
    "            # 计算在视域范围内的数据点·的数量\n",
    "            # visible_points = tag_df[(tag_df[\"Y(PPM)\"] >= xlim[0]) & \n",
    "            #                         (tag_df[\"Y(PPM)\"] <= xlim[1]) & \n",
    "            #                         (tag_df['ALL_Alkaline'] >= ylim[0]) & \n",
    "            #                         (tag_df['ALL_Alkaline'] <= ylim[1])]\n",
    "\n",
    "            # num_visible_points = len(visible_points)\n",
    "            num_visible_points = len(tag_df)\n",
    "\n",
    "            # 在图上显示可见的数据点的数量\n",
    "            ax.text(0.05, 0.95, f'Used points: {num_visible_points}', transform=ax.transAxes, verticalalignment='top',fontsize=14)\n",
    "\n",
    "        fig.tight_layout()\n",
    "            \n",
    "\n",
    "        with open(output_dir+'/'+'Pearce_Base_' + Type + '_Withlines.pkl', 'wb') as f:\n",
    "            pickle.dump(fig, f)\n",
    "\n",
    "    all_end_time = time.time()\n",
    "\n",
    "    all_time_taken = all_end_time - begin_time\n",
    "\n",
    "\n",
    "    # 保存图，包含图例\n",
    "    # 创建存图的文件夹\n",
    "    fig.savefig(output_dir+'/'+'Pearce_Base_' + Type + '.svg')\n",
    "    # fig.savefig(output_dir+'/'+'Pearce_Base_' + Type + '.pdf')\n",
    "    fig.savefig(output_dir+'/'+'Pearce_Base_' + Type + '.jpg', dpi=600)\n",
    "    \n",
    "    conn.close()\n",
    "    print(f\"All time taken: {all_time_taken:.3f} seconds\")\n",
    "    return(fig)\n",
    "    \n",
    "\n",
    "# 文件名\n",
    "filename = 'GeoRoc.db'\n",
    "Type = 'Granite'\n",
    "color_config_dir = 'Color_Config'\n",
    "if not os.path.exists(color_config_dir):\n",
    "    os.makedirs(color_config_dir)\n",
    "output_dir = 'Pearce'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "Pearce_base(filename, 'Granite', output_dir = 'Pearce')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
